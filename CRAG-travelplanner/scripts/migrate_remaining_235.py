#!/usr/bin/env python3
"""
è¡¥å…¨å‰©ä½™235æ¡POIè®°å½•
===================
ä»ç´¢å¼•5690åˆ°5924çš„POIæ•°æ®è¿ç§»
"""

import os
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
from openai import OpenAI
from langchain_core.embeddings import Embeddings
from langchain_oceanbase.vectorstores import OceanbaseVectorStore
from langchain_core.documents import Document
import json
import uuid
import time
from tqdm import tqdm

from seekdb_agent.db.sparse_encoder import TFIDFEncoder

load_dotenv()

# æ•°æ®æ–‡ä»¶
DATA_FILE = Path(__file__).parent.parent / "data" / "pois_export.json"

# OceanBaseè¿æ¥
OB_CONFIG = {
    "host": os.getenv("DATABASE_HOST", "127.0.0.1"),
    "port": int(os.getenv("DATABASE_PORT", "2881")),
    "user": os.getenv("DATABASE_USER", "root@test"),
    "password": os.getenv("DATABASE_PASSWORD", ""),
    "db_name": os.getenv("DATABASE_NAME", "crag_travelplanner"),
}

EMBEDDING_DIM = int(os.getenv("EMBEDDING_DIM", "1024"))


class DashScopeEmbeddings(Embeddings):
    """é˜¿é‡Œäº‘DashScope Embedding"""

    def __init__(self, model: str = "text-embedding-v4"):
        self.model = model
        self.client = OpenAI(
            api_key=os.getenv("EMBEDDING_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        """Embedå¤šä¸ªæ–‡æ¡£"""
        embeddings = []
        for text in texts:
            response = self.client.embeddings.create(model=self.model, input=text)
            embeddings.append(response.data[0].embedding)
        return embeddings

    def embed_query(self, text: str) -> list[float]:
        """Embedå•ä¸ªæŸ¥è¯¢"""
        response = self.client.embeddings.create(model=self.model, input=text)
        return response.data[0].embedding


def prepare_data(pois: list[dict]) -> tuple[list[Document], list[str], list[dict[int, float]]]:
    """å‡†å¤‡æ•°æ®ï¼ˆä¸ä¸»è„šæœ¬ç›¸åŒçš„é€»è¾‘ï¼‰"""
    documents = []
    fulltext_content = []

    for poi in pois:
        parts = [poi["name"]]
        if poi.get("city"):
            parts.append(f"{poi['city']}, {poi.get('state', '')}")
        if poi.get("primary_category"):
            parts.append(poi["primary_category"])
        if poi.get("editorial_summary"):
            parts.append(poi["editorial_summary"])

        # Add descriptive attributes for better fulltext search
        if poi.get("rating"):
            rating = float(poi["rating"])
            rating_desc = "excellent rating" if rating >= 4.5 else "good rating" if rating >= 4.0 else ""
            if rating_desc:
                parts.append(rating_desc)

        if poi.get("reviews_count"):
            reviews = int(poi["reviews_count"])
            if reviews >= 10000:
                parts.append("very popular destination")
            elif reviews >= 1000:
                parts.append("popular destination")

        if poi.get("price_level"):
            price_level = int(poi["price_level"])
            price_desc = {
                1: "low price",
                2: "moderate price",
                3: "high price",
                4: "high price",
            }.get(price_level, "")
            if price_desc:
                parts.append(price_desc)

        text = ". ".join(filter(None, parts))

        metadata = {
            "id": str(poi["id"]),
            "name": poi["name"],
            "city": poi.get("city"),
            "state": poi.get("state"),
            "latitude": float(poi["latitude"]) if poi.get("latitude") else None,
            "longitude": float(poi["longitude"]) if poi.get("longitude") else None,
            "rating": float(poi["rating"]) if poi.get("rating") else None,
            "reviews_count": poi.get("reviews_count"),
            "price_level": poi.get("price_level"),
            "primary_category": poi.get("primary_category"),
        }

        documents.append(Document(page_content=text, metadata=metadata))
        fulltext_content.append(text)

    # éœ€è¦ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒTF-IDFä»¥ä¿æŒè¯æ±‡è¡¨ä¸€è‡´
    print("      åŠ è½½å®Œæ•´æ•°æ®é›†è®­ç»ƒTF-IDF...")
    with open(DATA_FILE) as f:
        all_pois = json.load(f)

    all_texts = []
    for poi in all_pois:
        parts = [poi["name"]]
        if poi.get("city"):
            parts.append(f"{poi['city']}, {poi.get('state', '')}")
        if poi.get("primary_category"):
            parts.append(poi["primary_category"])
        if poi.get("editorial_summary"):
            parts.append(poi["editorial_summary"])
        all_texts.append(". ".join(filter(None, parts)))

    print("      è®­ç»ƒTF-IDF...")
    tfidf = TFIDFEncoder(max_vocab_size=100000)
    tfidf.fit(all_texts)

    # åªä¸ºå‰©ä½™235æ¡ç”Ÿæˆsparse embeddings
    sparse_embeddings = [tfidf.encode(doc.page_content) for doc in documents]
    print(f"      è¯æ±‡è¡¨: {tfidf.get_vocab_size()}è¯")

    return documents, fulltext_content, sparse_embeddings


def main():
    print("=" * 60)
    print("è¡¥å…¨å‰©ä½™235æ¡POI")
    print("=" * 60)

    # 1. åŠ è½½æœ€å235æ¡è®°å½•
    print("\n[1/3] åŠ è½½å‰©ä½™POIæ•°æ®...")
    with open(DATA_FILE) as f:
        all_pois = json.load(f)

    remaining_pois = all_pois[5690:]  # ä»ç´¢å¼•5690å¼€å§‹
    print(f"      åŠ è½½ {len(remaining_pois)} æ¡POI (ç´¢å¼•5690-5924)")

    # 2. å‡†å¤‡æ•°æ®
    print("\n[2/3] å‡†å¤‡æ•°æ®...")
    documents, fulltext_content, sparse_embeddings = prepare_data(remaining_pois)

    # 3. è¿æ¥OceanBaseå¹¶è¿ç§»
    print("\n[3/3] è¿ç§»å‰©ä½™æ•°æ®...")

    embeddings = DashScopeEmbeddings(model=os.getenv("EMBEDDING_MODEL", "text-embedding-v4"))
    store = OceanbaseVectorStore(
        connection_args=OB_CONFIG,
        table_name="pois",
        embedding_function=embeddings,
        embedding_dim=EMBEDDING_DIM,
        include_sparse=True,
        include_fulltext=True,
        drop_old=False,  # ä¸åˆ é™¤å·²æœ‰æ•°æ®
    )

    # å°æ‰¹æ¬¡æ’å…¥ï¼Œé¿å…è¶…æ—¶
    batch_size = 10
    total = len(documents)

    with tqdm(total=total, desc="      è¿ç§»POI", unit="æ¡") as pbar:
        for i in range(0, total, batch_size):
            end = min(i + batch_size, total)
            batch_docs = documents[i:end]
            batch_fulltext = fulltext_content[i:end]
            batch_sparse = sparse_embeddings[i:end]

            store.add_documents_with_hybrid_fields(
                documents=batch_docs,
                sparse_embeddings=batch_sparse,
                fulltext_content=batch_fulltext
            )

            pbar.update(len(batch_docs))

            # æ‰¹æ¬¡é—´å»¶è¿Ÿé¿å…è¶…æ—¶
            if end < total:
                time.sleep(0.5)

    print("\n" + "=" * 60)
    print("âœ… è¡¥å…¨å®Œæˆï¼")
    print("=" * 60)
    print(f"   æ–°å¢è®°å½•æ•°: {len(documents)}")
    print(f"   Vector: {EMBEDDING_DIM}ç»´")
    print(f"   Sparse + Fulltext: å·²å¯ç”¨")

    # éªŒè¯æ€»æ•°
    import pymysql
    conn = pymysql.connect(
        host=OB_CONFIG["host"],
        port=OB_CONFIG["port"],
        user=OB_CONFIG["user"],
        password=OB_CONFIG["password"],
        database=OB_CONFIG["db_name"]
    )
    cursor = conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM pois")
    final_count = cursor.fetchone()[0]
    conn.close()

    print(f"\nğŸ“Š å½“å‰æ•°æ®åº“æ€»è®°å½•æ•°: {final_count}/5925")
    if final_count == 5925:
        print("ğŸ‰ æ‰€æœ‰POIå·²æˆåŠŸè¿ç§»ï¼")
    else:
        print(f"âš ï¸  è¿˜ç¼ºå°‘ {5925 - final_count} æ¡è®°å½•")


if __name__ == "__main__":
    main()
